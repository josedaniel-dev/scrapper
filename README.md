# ğŸ•¸ï¸ Scrape & Classify â€“ Web Data Extraction Pipeline

A modular Python pipeline for extracting, cleaning, and classifying web data using open-source tools. Designed for smart web prospecting, offline research, and scalable automation.

---

## ğŸ“Œ Features

* ğŸ” **Web Scraping**: Collects structured data from target websites using `requests` and `BeautifulSoup`.
* ğŸ§¹ **Data Cleaning**: Standardizes and prepares text for analysis using `pandas`, `re`, and `unicodedata`.
* ğŸ§  **ML Classification**: Predicts categories or relevance using a trained `scikit-learn` classifier.
* ğŸ—ƒï¸ **Export Options**: Saves results to `.csv`, `.json`, and `.txt` formats for integration and sharing.
* ğŸ” **Offline-First**: Fully operable without internet after initial setup. No external API dependencies.
* ğŸ“ **Modular Design**: Easy to adapt, reuse, or extend in parts or as a full pipeline.

---

## ğŸš€ Use Cases

* Local business discovery and qualification
* Lead generation and client research
* Academic data collection and preprocessing
* Custom classifiers for specific domains or keywords

---

## ğŸ§° Stack

| Function          | Tools Used                         |
| ----------------- | ---------------------------------- |
| Scraping          | `requests`, `BeautifulSoup`        |
| Cleaning          | `pandas`, `re`, `unicodedata`      |
| ML Classification | `scikit-learn`, `joblib`           |
| Exporting         | `csv`, `json`, plain text handling |
| Logging & CLI     | `argparse`, `logging`, `.env`      |

---

## ğŸ› ï¸ How It Works

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Run the scraper (customize inside config or via CLI)
python scrape.py --target "https://example.com" --output data/raw.csv

# 3. Clean and preprocess the scraped data
python clean.py --input data/raw.csv --output data/clean.csv

# 4. Classify the cleaned data using the trained model
python classify.py --input data/clean.csv --model models/classifier.pkl --output results/predictions.csv
```

Each step is independent and can be used modularly.

---

## ğŸ“Š Example Output

```csv
name,website,description,category,score
"ACME Bakery","acme.com","...","food",0.92
"FutureTech Solutions","futuretech.ai","...","tech",0.88
...
```

---

## ğŸ“‚ Folder Structure

```
scrape_classify_pipeline/
â”‚
â”œâ”€â”€ data/                 # Raw and processed data
â”œâ”€â”€ models/               # Trained models (.pkl)
â”œâ”€â”€ results/              # Classification outputs
â”œâ”€â”€ scrape.py             # Web scraping logic
â”œâ”€â”€ clean.py              # Data cleaning script
â”œâ”€â”€ classify.py           # ML classification pipeline
â”œâ”€â”€ utils/                # Reusable utility functions
â”œâ”€â”€ config.env            # Environment variables
â””â”€â”€ README.md             # Project overview
```

---

## ğŸ“¦ Dependencies

* Python 3.8+
* pandas
* beautifulsoup4
* scikit-learn
* requests
* joblib

---

## ğŸ”’ Privacy-First Design

This pipeline does **not** rely on cloud APIs or send data externally. Everything runs locally, making it ideal for private research or restricted environments.

---

## ğŸ“Œ License

Licensed under the MIT License â€“ feel free to modify and use for personal or commercial projects.

---

## ğŸ™Œ Author

**Jose Daniel Soto**
[ğŸ“§ Email](mailto:jdsotomarin@gmail.com) | [ğŸŒ GitHub](https://github.com/josedaniel-dev/portfolio) | [ğŸ”— LinkedIn](https://linkedin.com/n/josedanielsoto)

